# Example configuration for text-generation-webui (oobabooga)

# LLM Provider Configuration
LLM_PROVIDER=local

# text-generation-webui Configuration
LOCAL_BASE_URL=http://localhost:5000
LOCAL_MODEL=Llama-3.1-8B-Instruct

# Note: The model name should match what you have loaded in text-generation-webui
# Common models:
# LOCAL_MODEL=Llama-3.1-8B-Instruct
# LOCAL_MODEL=Llama-3.2-3B-Instruct
# LOCAL_MODEL=Mistral-7B-Instruct-v0.3
# LOCAL_MODEL=CodeLlama-7B-Instruct
# LOCAL_MODEL=Phi-3-Medium-4K-Instruct

# Google API Configuration (still required for document fetching)
GOOGLE_APPLICATION_CREDENTIALS=path/to/your/credentials.json