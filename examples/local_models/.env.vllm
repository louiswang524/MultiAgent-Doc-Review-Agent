# Example configuration for vLLM (local OpenAI-compatible server)

# LLM Provider Configuration
LLM_PROVIDER=local

# vLLM Configuration
LOCAL_BASE_URL=http://localhost:8000
LOCAL_MODEL=meta-llama/Llama-3.1-8B-Instruct

# Alternative models you can serve with vLLM:
# LOCAL_MODEL=meta-llama/Llama-3.2-3B-Instruct     # Smaller model
# LOCAL_MODEL=meta-llama/Llama-3.1-70B-Instruct    # Larger model
# LOCAL_MODEL=mistralai/Mistral-7B-Instruct-v0.3   # Mistral 7B
# LOCAL_MODEL=microsoft/Phi-3-medium-4k-instruct   # Microsoft Phi-3
# LOCAL_MODEL=google/gemma-2-9b-it                 # Google Gemma 2

# Google API Configuration (still required for document fetching)
GOOGLE_APPLICATION_CREDENTIALS=path/to/your/credentials.json