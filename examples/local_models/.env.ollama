# Example configuration for Ollama local LLM setup

# LLM Provider Configuration
LLM_PROVIDER=ollama

# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b

# Alternative models you can try:
# OLLAMA_MODEL=llama3.2:3b          # Smaller, faster model
# OLLAMA_MODEL=llama3.1:70b         # Larger, more capable model (requires more memory)
# OLLAMA_MODEL=mistral:7b           # Mistral 7B model
# OLLAMA_MODEL=codellama:7b         # Code-specialized Llama model
# OLLAMA_MODEL=phi3:3.8b            # Microsoft Phi-3 model
# OLLAMA_MODEL=gemma2:9b            # Google Gemma 2 model

# Google API Configuration (still required for document fetching)
GOOGLE_APPLICATION_CREDENTIALS=path/to/your/credentials.json